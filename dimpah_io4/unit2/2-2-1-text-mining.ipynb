{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mining\n",
    "Prerequisite: Technicalities\n",
    "Wikidata API, other APIs\n",
    "Scraping HTML to extract structured/semi-structured information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Processing text\n",
    "Rule based (regex, etc)\n",
    "PoS tagging\n",
    "N-grams, co-occurrences and concordancer\n",
    "Word embeddings (word2vec, BERT, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Text Pre-processing\n",
    "\n",
    "- Rule-based Techniques (regex, etc)\n",
    "- Part-of-Speech (PoS) tagging\n",
    "- Chunking\n",
    "- N-grams, co-occurrences and concordancer\n",
    "- Word embeddings (word2vec, BERT, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Programming Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **text** as nothing more than a sequence of **words** and **punctuation**. In the following example, `sentence` followed by the `=` sign, and then some quoted words, separated with commas, surrounded with brackets is known as a list in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"History\", \"is\", \"full\", \"of\", \"people\", \"who\", \"knew\", \"about\", \"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can print the contents of the `sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['History', 'is', 'full', 'of', 'people', 'who', 'knew', 'about', 'history']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask for its length to find how many words are in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this exercise, we define two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['History', 'is', 'full', 'of', 'people', 'who', 'knew', 'about', 'history']\n",
      "['but', 'kept', 'on', 'repeating', 'the', 'same', 'mistakes', 'again', 'and', 'again', 'anyway']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = [\"History\", \"is\", \"full\", \"of\", \"people\", \"who\", \"knew\", \"about\", \"history\"]\n",
    "sentence2 = [\"but\", \"kept\", \"on\", \"repeating\", \"the\", \"same\", \"mistakes\", \"again\", \"and\", \"again\", \"anyway\"]\n",
    "\n",
    "print(sentence1)\n",
    "print(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two lists creates a new list with everything from the first list, followed by everything from the second list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['History', 'is', 'full', 'of', 'people', 'who', 'knew', 'about', 'history', 'but', 'kept', 'on', 'repeating', 'the', 'same', 'mistakes', 'again', 'and', 'again', 'anyway']\n"
     ]
    }
   ],
   "source": [
    "sentence = sentence1 + sentence2\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a new item to the list. When we `append()` to a list, the list itself is updated as a result of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['History', 'is', 'full', 'of', 'people', 'who', 'knew', 'about', 'history', 'but', 'kept', 'on', 'repeating', 'the', 'same', 'mistakes', 'again', 'and', 'again', 'anyway', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence.append(\".\")\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify the elements of a Python list by their order of occurrence in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History\n"
     ]
    }
   ],
   "source": [
    "print(sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n"
     ]
    }
   ],
   "source": [
    "print(sentence[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract a *slice* or *sublist* from `sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'full', 'of', 'people']\n"
     ]
    }
   ],
   "source": [
    "print(sentence[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any individual word in the `sentence` is a `word` of type *String* (short *str*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "word = \"history\"\n",
    "\n",
    "print(word)\n",
    "print(type(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Natural Language Processing (NLP) Programming Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deeper introduction into NLP programming techniques, we continue with the usage of NLTK. [NLTK](https://www.nltk.org/) is a widely used standard Natural Language Processing (NLP) and Computational Linguistics (CL) Python library with prebuilt functions and utilities for the ease of use and implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can check the count and frequency distributions of words in a text. For example, if we take the previous `sentence`, we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('again', 2), ('History', 1), ('is', 1), ('full', 1), ('of', 1), ('people', 1), ('who', 1), ('knew', 1), ('about', 1), ('history', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(sentence)\n",
    "\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we first invoke [FreqDist](http://www.nltk.org/api/nltk.html?highlight=freqdist), we pass the name of the text as an argument. The expression `most_common(10`) gives us a list of the 10 most frequently occurring types in the text (in this case, it will print all the words in the `sentence` since the length of the `sentence` is less than 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Collocations and N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *collocation* is a sequence of words that occur together unusually often. To understand collocations, we start by extracting from the `sentence` a list of word pairs, also known as bigrams. The *bigrams* are lists of two words (*bi*), while n-grams are lists of *n* words (e.g. ..). This is easily accomplished with the function `bigrams()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('History', 'is'), ('is', 'full'), ('full', 'of'), ('of', 'people'), ('people', 'who'), ('who', 'knew'), ('knew', 'about'), ('about', 'history'), ('history', 'but'), ('but', 'kept'), ('kept', 'on'), ('on', 'repeating'), ('repeating', 'the'), ('the', 'same'), ('same', 'mistakes'), ('mistakes', 'again'), ('again', 'and'), ('and', 'again'), ('again', 'anyway'), ('anyway', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "list_of_bigrams = list(bigrams(sentence))\n",
    "\n",
    "print(list_of_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word and sentence tokenization.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(\"History is full of people who knew about history. But they kept on repeating the same mistakes again and again anyway.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['History is full of people who knew about history.',\n",
       " 'But they kept on repeating the same mistakes again and again anyway.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(\"History is full of people who knew about history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['History', 'is', 'full', 'of', 'people', 'who', 'knew', 'about', 'history']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Part-of-speech (PoS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('History', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('full', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('people', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('knew', 'VBD'),\n",
       " ('about', 'IN'),\n",
       " ('history', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('kept', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('repeating', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('same', 'JJ'),\n",
       " ('mistakes', 'NNS'),\n",
       " ('again', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('again', 'RB'),\n",
       " ('anyway', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that `\"and\"` is CC, a coordinating conjunction; `\"again\"` and `\"anyway\"` are RB, or adverbs; `\"about` is IN, a preposition; `\"History\"`, `\"history` are NN, nouns; `\"people\"` and `\"mistakes\"` are plural nouns; `\"full\"` and `\"same\"` are JJ, adjectives; `\"is\"` is VBZ, a verb, 3rd person singular present; `\"kept\"` is VBD, a verb, past tense; `\"repeating\"` is VBG, a verb, gerund/present participle, `\"who\"` is WP, a wh-pronoun, and `\"determiner\"` is DT, a determiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "from nltk import help\n",
    "\n",
    "help.upenn_tagset(\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Chunking\n",
    "\n",
    "Chunking segments texts and depends on PoS tagging. Like tokenization, which omits whitespace, chunking usually selects a subset of the tokens. Also like tokenization, the pieces produced by a chunker do not overlap in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
