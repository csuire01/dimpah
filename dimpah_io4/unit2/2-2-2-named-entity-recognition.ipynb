{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Named Entity Recognition (NER)\n",
    "\n",
    "Named entity recognition (NER) task aims at identifying real-world entities, such as names of people, organizations, and locations within historical documents. The term of *named entity (NE)*, widely used in Information Extraction (IE) or other Natural Language Processing (NLP) applications, was born in the Message Understanding Conferences (MUC) which influenced the IE research between 1988 and 1996. Since 1999, the yearly conference on\n",
    "Natural Language Learning (CoNLL) covers a large framework of topics about NLP, mostly through machine learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What are named entities?\n",
    "\n",
    "Named entities are generally proper nouns that refer to specific entities that can be a person, organization, location, date, etc. If we consider this sentence as an example: *Mount Everest is the tallest mountain above sea level*, NER should detect *Mount Everest* as a named entity of type location as it refers to a specific entity.\n",
    "\n",
    "Some other examples of named entities are listed in the following table:\n",
    "\n",
    "\n",
    "|  | Named Entity  |  \n",
    "|-----|---|\n",
    "|  ORGANIZATION   | United Nations Organization, UNICEF, Microsoft |\n",
    "|  PERSON   | Novak Djokovic, Beyonc√©, Scarlett Johansson |\n",
    "|  LOCATION   |  Mount Everest, River Nile, Machu Picchu Archaeological Park  |\n",
    "|  DATE   |  3rd April 1988, 7 June  |\n",
    "|  TIME   | 8:45 A.M., one-thirty am |\n",
    "|  GPE   |  France, Liechtenstein, Democratic Republic of Congo |\n",
    "|  MONEY   |  7 million dollars, 73.01 INR |\n",
    "\n",
    "What should be considered as a named entity (NE) in a text is quite open for discussion and depends on the kind of information one wants to extract. However, the set of named entity classes that is widely used contains the three fundamental entity types, person (PER), location (LOC), and organization (ORG), collectively referred to as the\n",
    "enamex since the MUC-6 competition ([Grishman et al 1996](https://aclanthology.org/C96-1079.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Why are named entities important? (case studies)\n",
    "\n",
    "The detection of entities can be considered as a first step in the exploration of data collections. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying content for news providers\n",
    "\n",
    "News publishers generate large amounts of content on a daily basis and managing them correctly is very important to get the most use of each article. NER can automatically scan an entire collection of articles and reveal which are the major people, organizations, and places discussed in them. Knowing these relevant information may help in automatically categorizing the articles in defined hierarchies and enable smooth content discovery. This could also save a lot of time and boost the efficiency of teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating customer support\n",
    "\n",
    "There are a number of ways to make the process of customer feedback handling smooth and NER could be one of them. For example, the customer support department of an electronic store should handle multiple branches worldwide, thus it needs to go through a number mentions in the customer feedback comments. NER could provide entities as locations and products, and these can be then used to categorize the complaint and assign it to the relevant department within the organization that should be handling this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring historical documents\n",
    "\n",
    "Historical newspapers are considered more and more as an important source of historical knowledge. As the amount of digitized data accumulates, tools for harvesting the data are needed to gather information. Tools like NER can be extremely valuable to researchers, historians, or librarians for adding structure to the volumes of unstructured data and for improving access to the historical digitized collections.  For example, a simple keyword search can already provide a historian with a sense of whether a collection contains material relevant for their research, thus saving many hours of visiting archives and skimming through pages. NER task can be used to detect person names and locations, these entities having an equally significant presence in the news domain, in which people are often at the core of the events reported in articles. For exampl, the EU's digital platform for cultural heritage, [Europeana](http://www.europeana-newspapers.eu/named-entity-recognition-for-digitised-newspapers/), is using NER to make historical newspapers searchable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting valuable information from medical documents\n",
    "\n",
    "Electronic health records are a valuable source of routinely collected health data that can be used for secondary purposes, including clinical and epidemiological research. They typically contain information on consultations, admissions, symptoms, clinical examinations, test results, diagnoses, treatments, and outcomes. NER can clinic letters or discharge summaries can ease the process of information extraction from free-text sources of prescription information, such as clinic letters or discharge summaries. In this case, the NER task can involve extracting different types of entities: drug, strength, duration, route, form, dosage, frequency, reason of administration, etc. NER can also recognize and match demographic factors that could provide analysts/doctors deeper insights.\n",
    "\n",
    "Similar to MUC, another known competition initiated in 2004 by the Informatics for Integrating Biology and the Bedside ([i2b2](https://www.i2b2.org/)) was designed to encourage the development of NLP techniques for the extraction of medication-related information from narrative patient records, in order to accelerate the translation of clinical findings into novel diagnostics and prognostics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aiding risk assessment for financial institutions\n",
    "\n",
    "Risk assessment is a crucial activity for financial institutions because it helps them to determine the amount of capital they should hold to assure their stability. Manual extraction of relevant information from text-based financial documents is expensive and time-consuming. NER can extract credit risk attributes from a large volume of *live* financial documents, numbering in the millions of documents for a large bank financial documents. In the financial domain, example named entity types are: lenger, borrower, amount, date, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easing the research process\n",
    "\n",
    "An online journal or conference publication site could hold millions of research papers and scholarly articles. There can be hundreds of papers on a single topic with slight modifications. Organizing all this data in a well-structured manner can be complex. Segregating the papers on the basis of the relevant entities it holds can save the trouble of going through the plethora of information on the subject matter. For instance, if the articles have in their metadata different types of entities (for example, NER can detect fields of study as *Named Entity Recognition* and *Information Extraction*), one can quickly find the articles where the use of *named entity recognition in historical documents* is discussed. This, NER could enable students and researchers to find relevant material faster by summarizing papers and archive material and highlighting key terms, topics, and themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Named Entity Recognition with NLTK & spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we explore the task of Named Entity Recognition (NER) *tagging* of sentences. *Tagging* (or *labelling*) means the detection of entities in text and the correct assignment of an entity type of them. In NLP, an *entity* is a sequence of one or more words (*tokens*). Thus, the task is to tag each token in a given sentence with an appropriate tag such as Person, Location, etc.\n",
    "\n",
    "For detecting entities with NLP programming techniques, we continue with the usage of two libraries: NLTK and spaCy. [NLTK](https://www.nltk.org/) is a widely used standard Natural Language Processing (NLP) and Computational Linguistics (CL) Python library with prebuilt functions and utilities for the ease of use and implementation. [spaCy](https://spacy.io/) is an open-source software Python library for advanced natural language processing that covers multiple NLP tasks (part-of-speech tagging, named entity recognition, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase1 = \"In 1979, after more than a century of vaccination campaigns around the planet, the World Health Organization certified that smallpox had been eradicated\"\n",
    "    \n",
    "phrase2 = \"Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam. Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1979, after more than a century of vaccination campaigns around the planet, the World Health Organization certified that smallpox had been eradicated'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam. Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. NER with [NLTK](https://www.nltk.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: In 1979, after more than a century of vaccination campaigns around the planet, the World Health Organization certified that smallpox had been eradicated\n",
      "(S\n",
      "  In/IN\n",
      "  1979/CD\n",
      "  ,/,\n",
      "  after/IN\n",
      "  more/JJR\n",
      "  than/IN\n",
      "  a/DT\n",
      "  century/NN\n",
      "  of/IN\n",
      "  vaccination/NN\n",
      "  campaigns/NNS\n",
      "  around/IN\n",
      "  the/DT\n",
      "  planet/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION World/NNP)\n",
      "  Health/NNP\n",
      "  Organization/NNP\n",
      "  certified/VBD\n",
      "  that/IN\n",
      "  smallpox/NN\n",
      "  had/VBD\n",
      "  been/VBN\n",
      "  eradicated/VBN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "sentences = sent_tokenize(phrase1)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence:\", sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags_sentence = pos_tag(words)\n",
    "    ne_chunks_sentence = ne_chunk(pos_tags_sentence)\n",
    "    print(ne_chunks_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam.\n",
      "(S\n",
      "  Beginning/VBG\n",
      "  in/IN\n",
      "  February/NNP\n",
      "  1965/CD\n",
      "  ,/,\n",
      "  there/EX\n",
      "  were/VBD\n",
      "  8/CD\n",
      "  weeks/NNS\n",
      "  of/IN\n",
      "  unbroken/JJ\n",
      "  bombing/NN\n",
      "  by/IN\n",
      "  (GPE U.S./NNP)\n",
      "  forces/NNS\n",
      "  of/IN\n",
      "  targets/NNS\n",
      "  in/IN\n",
      "  (GPE North/NNP Vietnam/NNP)\n",
      "  ./.)\n",
      "Sentence: Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.\n",
      "(S\n",
      "  Over/IN\n",
      "  the/DT\n",
      "  next/JJ\n",
      "  three/CD\n",
      "  years/NNS\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (GPE Unites/NNP States/NNPS)\n",
      "  dropped/VBD\n",
      "  more/JJR\n",
      "  bombs/NNS\n",
      "  than/IN\n",
      "  were/VBD\n",
      "  dropped/VBN\n",
      "  over/IN\n",
      "  (GPE Asia/NNP)\n",
      "  and/CC\n",
      "  (GPE Europe/NNP)\n",
      "  during/IN\n",
      "  World/NNP\n",
      "  War/NNP\n",
      "  II/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(phrase2)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence:\", sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags_sentence = pos_tag(words)\n",
    "    ne_chunks_sentence = ne_chunk(pos_tags_sentence)\n",
    "    print(ne_chunks_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. NER with [spaCy](https://spacy.io/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy currently provides support for the following [languages](https://spacy.io/usage/models). For applying NER for an English text, we need to download a spaCy NER model for [English](https://spacy.io/models/en). We choose to download `en_core_web_sm` because it is the smallest one in regards to download size (12Mb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: 1979 --- Entity Type (tag/label): DATE\n",
      "Entity: more than a century --- Entity Type (tag/label): DATE\n",
      "Entity: the World Health Organization --- Entity Type (tag/label): ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(phrase1)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print('Entity:', entity.text, '---', 'Entity Type (tag/label):', entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: February 1965 --- Entity Type: DATE\n",
      "Entity: 8 weeks --- Entity Type: DATE\n",
      "Entity: U.S. --- Entity Type: GPE\n",
      "Entity: North Vietnam --- Entity Type: LOC\n",
      "Entity: the next three years --- Entity Type: DATE\n",
      "Entity: the Unites States --- Entity Type: GPE\n",
      "Entity: Asia --- Entity Type: LOC\n",
      "Entity: Europe --- Entity Type: LOC\n",
      "Entity: World War II --- Entity Type: EVENT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(phrase2)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print('Entity:', entity.text, '---',  'Entity Type:', entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 : Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam.\n",
      "Sentence 1 : Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print(\"Sentence\", i, ':', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IOB format (short for **i**nside, **o**utside, **b**eginning) is a common tagging format for tagging tokens in a chunking task in computational linguistics (ex. named entity recognition) to describe the entity boundaries. In IOB, the I- prefix before a tag indicates that the tag is inside a chunk. An O tag indicates that a token belongs to no chunk. The B- prefix before a tag indicates that the tag is the beginning of a chunk that immediately follows another chunk without O tags between them.\n",
    "\n",
    "|  | Named Entity  |  \n",
    "|-----|---|\n",
    "|  I   | An inner token of a multi-token entity |\n",
    "|  O   |  A non-entity token  |\n",
    "|  B   | The beginning token of a multi-token entity |\n",
    "\n",
    "| U.S.  | dropped | more | bombs | than | over | Asia  | and | Europe | during | World | War | II\n",
    "|---------|--------|---------|------|-------|------|---------|------|------|-------|-----|--------|--------|\n",
    "|  I-GPE       |    O    |    O     |  O    |    O   |   O   |   I-LOC      |   O   |  I-LOC    |    O   |   I-EVENT  |   I-EVENT     |    I-EVENT    |\n",
    "\n",
    "\n",
    "Another similar format which is widely used is IOB2 format, which is the same as the IOB format except that the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag). \n",
    "\n",
    "| U.S.  | dropped | more | bombs | than | over | Asia  | and | Europe | during | World | War | II\n",
    "|---------|--------|---------|------|-------|------|---------|------|------|-------|-----|--------|--------|\n",
    "|  B-GPE       |    O    |    O     |  O    |    O   |   O   |   B-LOC      |   O   |  B-LOC    |    O   |   B-EVENT  |   I-EVENT     |    I-EVENT    |\n",
    "\n",
    "\n",
    "Other tagging scheme is BIOES/BILOU, where 'E' and 'L' denotes Last or Ending token is such a sequence and 'S' denotes Single element or 'U' Unit element. \n",
    "\n",
    "|  | Named Entity  |  \n",
    "|-----|---|\n",
    "|  B   | The first token of a multi-token entity |\n",
    "|  I   | An inner token of a multi-token entity |\n",
    "|  L/E   |  The last/ending token of a multi-token entity  |\n",
    "|  O   |  A non-entity token  |\n",
    "|  U/S   | A single token entity |\n",
    "\n",
    "| U.S.  | dropped | more | bombs | than | over | Asia  | and | Europe | during | World | War | II\n",
    "|---------|--------|---------|------|-------|------|---------|------|------|-------|-----|--------|--------|\n",
    "|  S-GPE       |    O    |    O     |  O    |    O   |   O   |   S-LOC      |   O   |  S-LOC    |    O   |   B-EVENT  |   I-EVENT     |    E-EVENT    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [nltk.ne_chunk(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. How to build or train a NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "The CoNLL 2003 dataset consists of English newswire from the Reuters RCV1 corpus and it includes standard train, development, and test sets. This corpus consists of Reuters news stories between August 1996 and August 1997. For the training and development set, ten days' worth of data were taken from the files representing the end of August 1996. For the test set, the texts were from December 1996. The preprocessed raw data covers the month of September 1996.\n",
    "\n",
    "The learning methods will be trained with the training data. The development data should be used for tuning the parameters of the learning methods. The test data will be used to measuring the performance of the methods. \n",
    "\n",
    "The corpus is available with some linguistic preprocessing already done: for all data, a tokenizer, part-of-speech tagger, and a chunker were applied to the raw data. The data contains entities of four types: persons (PER),\n",
    "organizations (ORG), locations (LOC) and miscellaneous names (MISC). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data format (IOB tagging scheme)\n",
    "\n",
    "| word | part-of-speech  | chunk | entity type |  |\n",
    "|-----|---|---|---|---|\n",
    "|  U.N.  | NNP | I-NP | I-ORG | \n",
    "|  official |  NN |  I-NP  | O | \n",
    "|  Ekeus  | NNP |  I-NP  | I-PER | \n",
    "|  heads  | VBZ |  I-VP |  O | \n",
    "|  for  | IN |  I-PP |  O | \n",
    "|  Baghdad  | NNP  | I-NP  | I-LOC | \n",
    "|  . |  . |  O  | O | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>chunk</th>\n",
       "      <th>entity_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRICKET</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEICESTERSHIRE</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAKE</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OVER</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  pos chunk entity_type\n",
       "0         CRICKET  NNP  I-NP           O\n",
       "1               -    :     O           O\n",
       "2  LEICESTERSHIRE  NNP  I-NP       I-ORG\n",
       "3            TAKE  NNP  I-NP           O\n",
       "4            OVER   IN  I-PP           O"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "data = read_csv('data/2.2.2/data.txt', sep='\\t') \n",
    "# train_sents = list(nltk.corpus.conll2002.iob_sents('eng.train'))??\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>chunk</th>\n",
       "      <th>entity_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50721</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50722</th>\n",
       "      <td>--</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50723</th>\n",
       "      <td>Dhaka</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50724</th>\n",
       "      <td>Newsroom</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50725</th>\n",
       "      <td>880-2-506363</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  pos chunk entity_type\n",
       "50721             .    .     O           O\n",
       "50722            --    :     O           O\n",
       "50723         Dhaka  NNP  I-NP       I-ORG\n",
       "50724      Newsroom  NNP  I-NP       I-ORG\n",
       "50725  880-2-506363   CD  I-NP           O"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose the train, dev and test sets\n",
    "\n",
    "To build a well-performing machine learning (ML) model, it is essential to train the model on and test it against data that come from the same target distribution.\n",
    "\n",
    "- **Training dataset**: The actual dataset that we use to train the model. The model sees and learns from this data.\n",
    "- **Validation dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while training (*tuning model hyperparameters*). The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration. The validation set is used to evaluate a given model, but this is for frequent evaluation. The validation set is also known as the Dev set or the Development set. This makes sense since this dataset helps during the *development* stage of the model.\n",
    "- **Test dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). The test set is generally what is used to evaluate competing models. Many times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world.\n",
    "\n",
    "\n",
    "We will split our data using the `train_test_split` method from the Scikit-learn library. The procedure has one main configuration parameter, which is the size of the train and test sets. This is most commonly expressed as a percentage between 0 and 1 for either the train or test datasets. For example, a training set with the size of 0.67 (67 percent) means that the remainder percentage 0.33 (33 percent) is assigned to the test set.\n",
    "\n",
    "There is no optimal split percentage. Common split percentages include:\n",
    "\n",
    "    -Train: 80%, Dev: 10%, Test: 10%\n",
    "    -Train: 33%, Dev: 33%, Test: 33%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40575 5072 5072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(data['word'], data['entity_type'], test_size=0.20)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_dev, y_dev, test_size=0.50)\n",
    "\n",
    "print(len(X_train), len(X_dev), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_dev_features = vectorizer.transform(X_dev)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomial_naive_bayes = MultinomialNB(alpha=0.01)\n",
    "multinomial_naive_bayes.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The performance in this task is measured with **Precision** (P), **Recall** (R), and **F1** (F-measure). **Precision** is the percentage of named entities found by the learning system that are correct. **Recall** is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it is an exact match of the corresponding entity in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC     1.0000    0.0000    0.0000         1\n",
      "       I-LOC     0.8832    0.7632    0.8188       228\n",
      "      I-MISC     0.9041    0.5789    0.7059       114\n",
      "       I-ORG     0.8605    0.5468    0.6687       203\n",
      "       I-PER     0.9777    0.5418    0.6972       323\n",
      "           O     0.9317    0.9962    0.9629      4203\n",
      "\n",
      "    accuracy                         0.9292      5072\n",
      "   macro avg     0.9262    0.5711    0.6422      5072\n",
      "weighted avg     0.9290    0.9292    0.9217      5072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_dev_predictions = multinomial_naive_bayes.predict(X_dev_features)\n",
    "\n",
    "print(classification_report(y_pred=y_dev_predictions, y_true=y_dev, digits=4, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC     1.0000    0.0000    0.0000         1\n",
      "      I-MISC     0.9041    0.5789    0.7059       114\n",
      "       B-LOC     1.0000    1.0000    1.0000         0\n",
      "       I-LOC     0.8832    0.7632    0.8188       228\n",
      "       B-ORG     1.0000    1.0000    1.0000         0\n",
      "       I-ORG     0.8605    0.5468    0.6687       203\n",
      "       B-PER     1.0000    1.0000    1.0000         0\n",
      "       I-PER     0.9777    0.5418    0.6972       323\n",
      "\n",
      "   micro avg     0.9100    0.6053    0.7270       869\n",
      "   macro avg     0.9532    0.6788    0.7363       869\n",
      "weighted avg     0.9159    0.6053    0.7228       869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=y_dev_predictions, y_true=y_dev, digits=4, zero_division=1, \n",
    "                            labels=['B-MISC', 'I-MISC', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=500)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter=500)\n",
    "logistic_regression.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC     1.0000    0.0000    0.0000         1\n",
      "      I-MISC     0.9592    0.4123    0.5767       114\n",
      "       B-LOC     1.0000    1.0000    1.0000         0\n",
      "       I-LOC     0.9161    0.6228    0.7415       228\n",
      "       B-ORG     1.0000    1.0000    1.0000         0\n",
      "       I-ORG     0.8036    0.2217    0.3475       203\n",
      "       B-PER     1.0000    1.0000    1.0000         0\n",
      "       I-PER     0.9646    0.3375    0.5000       323\n",
      "\n",
      "   micro avg     0.9196    0.3947    0.5523       869\n",
      "   macro avg     0.9554    0.5743    0.6457       869\n",
      "weighted avg     0.9136    0.3947    0.5372       869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev_predictions = logistic_regression.predict(X_dev_features)\n",
    "\n",
    "print(classification_report(y_pred=y_dev_predictions, y_true=y_dev, digits=4, zero_division=1, \n",
    "                            labels=['B-MISC', 'I-MISC', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "neural_network = MLPClassifier()\n",
    "neural_network.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC     1.0000    0.0000    0.0000         1\n",
      "      I-MISC     0.8554    0.6228    0.7208       114\n",
      "       B-LOC     1.0000    1.0000    1.0000         0\n",
      "       I-LOC     0.8483    0.7851    0.8155       228\n",
      "       B-ORG     1.0000    1.0000    1.0000         0\n",
      "       I-ORG     0.8740    0.5468    0.6727       203\n",
      "       B-PER     1.0000    1.0000    1.0000         0\n",
      "       I-PER     0.9777    0.5418    0.6972       323\n",
      "\n",
      "   micro avg     0.8933    0.6168    0.7297       869\n",
      "   macro avg     0.9444    0.6871    0.7383       869\n",
      "weighted avg     0.9035    0.6168    0.7248       869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev_predictions = neural_network.predict(X_dev_features)\n",
    "\n",
    "print(classification_report(y_pred=y_dev_predictions, y_true=y_dev, digits=4, zero_division=1, \n",
    "                            labels=['B-MISC', 'I-MISC', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results on dev .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC     1.0000    1.0000    1.0000         0\n",
      "      I-MISC     0.8454    0.6457    0.7321       127\n",
      "       B-LOC     1.0000    1.0000    1.0000         0\n",
      "       I-LOC     0.7990    0.7756    0.7871       205\n",
      "       B-ORG     1.0000    1.0000    1.0000         0\n",
      "       I-ORG     0.8231    0.5194    0.6369       206\n",
      "       B-PER     1.0000    1.0000    1.0000         0\n",
      "       I-PER     0.9840    0.5987    0.7445       309\n",
      "\n",
      "   micro avg     0.8681    0.6293    0.7296       847\n",
      "   macro avg     0.9314    0.8174    0.8626       847\n",
      "weighted avg     0.8793    0.6293    0.7268       847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predictions = neural_network.predict(X_test_features)\n",
    "\n",
    "print(classification_report(y_pred=y_test_predictions, y_true=y_test, digits=4, zero_division=1, \n",
    "                            labels=['B-MISC', 'I-MISC', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General principles (ML, training data, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) State-of-the-art examples (list of SotA papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first end-to-end systems for sequence labeling tasks are based on pre-trained word and character embeddings encoded either by a bidirectional Long Short Term Memory (BiLSTM) network or a Convolutional Neural Network (CNN) ([Lample et al., 2016](https://arxiv.org/abs/1603.01360) and [Ma and Hovy 2016](https://arxiv.org/abs/1603.01354)), along with a Conditional Random Fields (CRF) decoder. One shortcoming of this type of model is that they were based on a single context-independent representation for each word. This problem has been further attenuated by methods based on language model pre-training that produced context-dependent word representations.\n",
    "These recent large-scale language models methods such as ELMo ([Peters et al. 2017](https://arxiv.org/abs/1705.00108)) and BERT ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)) further enhanced the performance of NER, yielding [state-of-the-art performances](http://nlpprogress.com/english/named_entity_recognition.html).\n",
    "\n",
    "- bidirectional Long Short Term Memory (BiLSTM) network [Lample et al., 2016](https://arxiv.org/abs/1603.01360)\n",
    "- Convolutional Neural Network (CNN) [Ma and Hovy 2016](https://arxiv.org/abs/1603.01354))\n",
    "- .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Use-case (mapping locations on a map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
