{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Named Entity Recognition (NER)\n",
    "\n",
    "Named entity recognition (NER) task aims at identifying real-world entities, such as names of people, organizations, and locations within historical documents. The term of *named entity (NE)*, widely used in Information Extraction (IE) or other Natural Language Processing (NLP) applications, was born in the Message Understanding Conferences (MUC) which influenced the IE research between 1988 and 1996. Since 1999, the yearly conference on\n",
    "Natural Language Learning (CoNLL) covers a large framework of topics about NLP, mostly through machine learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What are named entities?\n",
    "\n",
    "Named entities are generally proper nouns that refer to specific entities that can be a person, organization, location, date, etc. If we consider this sentence as an example: *Mount Everest is the tallest mountain above sea level*, NER should detect *Mount Everest* as a named entity of type location as it refers to a specific entity.\n",
    "\n",
    "Some other examples of named entities are listed in the following table:\n",
    "\n",
    "\n",
    "|  | Named Entity  |  \n",
    "|-----|---|\n",
    "|  ORGANIZATION   | United Nations Organization, UNICEF, Microsoft |\n",
    "|  PERSON   | Novak Djokovic, Beyonc√©, Scarlett Johansson |\n",
    "|  LOCATION   |  Mount Everest, River Nile, Machu Picchu Archaeological Park  |\n",
    "|  DATE   |  3rd April 1988, 7 June  |\n",
    "|  TIME   | 8:45 A.M., one-thirty am |\n",
    "|  GPE   |  France, Liechtenstein, Democratic Republic of Congo |\n",
    "|  MONEY   |  7 million dollars, 73.01 INR |\n",
    "\n",
    "What should be considered as a named entity (NE) in a text is quite open for discussion and depends on the kind of information one wants to extract. However, the set of named entity classes that is widely used contains the three fundamental entity types, person (PER), location (LOC), and organization (ORG), collectively referred to as the\n",
    "enamex since the MUC-6 competition ([Grishman et al 1996](https://aclanthology.org/C96-1079.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Why are named entities important? (case studies)\n",
    "\n",
    "The detection of entities can be considered as a first step in the exploration of data collections. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying content for news providers\n",
    "\n",
    "News publishers generate large amounts of content on a daily basis and managing them correctly is very important to get the most use of each article. NER can automatically scan an entire collection of articles and reveal which are the major people, organizations, and places discussed in them. Knowing these relevant information may help in automatically categorizing the articles in defined hierarchies and enable smooth content discovery. This could also save a lot of time and boost the efficiency of teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating customer support\n",
    "\n",
    "There are a number of ways to make the process of customer feedback handling smooth and NER could be one of them. For example, the customer support department of an electronic store should handle multiple branches worldwide, thus it needs to go through a number mentions in the customer feedback comments. NER could provide entities as locations and products, and these can be then used to categorize the complaint and assign it to the relevant department within the organization that should be handling this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring historical documents\n",
    "\n",
    "Historical newspapers are considered more and more as an important source of historical knowledge. As the amount of digitized data accumulates, tools for harvesting the data are needed to gather information. Tools like NER can be extremely valuable to researchers, historians, or librarians for adding structure to the volumes of unstructured data and for improving access to the historical digitized collections.  For example, a simple keyword search can already provide a historian with a sense of whether a collection contains material relevant for their research, thus saving many hours of visiting archives and skimming through pages. NER task can be used to detect person names and locations, these entities having an equally significant presence in the news domain, in which people are often at the core of the events reported in articles. For exampl, the EU's digital platform for cultural heritage, [Europeana](http://www.europeana-newspapers.eu/named-entity-recognition-for-digitised-newspapers/), is using NER to make historical newspapers searchable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting valuable information from medical documents\n",
    "\n",
    "Electronic health records are a valuable source of routinely collected health data that can be used for secondary purposes, including clinical and epidemiological research. They typically contain information on consultations, admissions, symptoms, clinical examinations, test results, diagnoses, treatments, and outcomes. NER can clinic letters or discharge summaries can ease the process of information extraction from free-text sources of prescription information, such as clinic letters or discharge summaries. In this case, the NER task can involve extracting different types of entities: drug, strength, duration, route, form, dosage, frequency, reason of administration, etc. NER can also recognize and match demographic factors that could provide analysts/doctors deeper insights.\n",
    "\n",
    "Similar to MUC, another known competition initiated in 2004 by the Informatics for Integrating Biology and the Bedside ([i2b2](https://www.i2b2.org/)) was designed to encourage the development of NLP techniques for the extraction of medication-related information from narrative patient records, in order to accelerate the translation of clinical findings into novel diagnostics and prognostics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aiding risk assessment for financial institutions\n",
    "\n",
    "Risk assessment is a crucial activity for financial institutions because it helps them to determine the amount of capital they should hold to assure their stability. Manual extraction of relevant information from text-based financial documents is expensive and time-consuming. NER can extract credit risk attributes from a large volume of *live* financial documents, numbering in the millions of documents for a large bank financial documents. In the financial domain, example named entity types are: lenger, borrower, amount, date, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easing the research process\n",
    "\n",
    "An online journal or conference publication site could hold millions of research papers and scholarly articles. There can be hundreds of papers on a single topic with slight modifications. Organizing all this data in a well-structured manner can be complex. Segregating the papers on the basis of the relevant entities it holds can save the trouble of going through the plethora of information on the subject matter. For instance, if the articles have in their metadata different types of entities (for example, NER can detect fields of study as *Named Entity Recognition* and *Information Extraction*), one can quickly find the articles where the use of *named entity recognition in historical documents* is discussed. This, NER could enable students and researchers to find relevant material faster by summarizing papers and archive material and highlighting key terms, topics, and themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Named Entity Recognition with NLTK & spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we explore the task of Named Entity Recognition (NER) *tagging* of sentences. *Tagging* (or *labelling*) means the detection of entities in text and the correct assignment of an entity type of them. In NLP, an *entity* is a sequence of one or more words (*tokens*). Thus, the task is to tag each token in a given sentence with an appropriate tag such as Person, Location, etc.\n",
    "\n",
    "For detecting entities with NLP programming techniques, we continue with the usage of two libraries: NLTK and spaCy. [NLTK](https://www.nltk.org/) is a widely used standard Natural Language Processing (NLP) and Computational Linguistics (CL) Python library with prebuilt functions and utilities for the ease of use and implementation. [spaCy](https://spacy.io/) is an open-source software Python library for advanced natural language processing that covers multiple NLP tasks (part-of-speech tagging, named entity recognition, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase1 = \"In 1979, after more than a century of vaccination campaigns around the planet, the World Health Organization certified that smallpox had been eradicated\"\n",
    "    \n",
    "phrase2 = \"Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam. Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1979, after more than a century of vaccination campaigns around the planet, the World Health Organization certified that smallpox had been eradicated'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam. Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. NER with [NLTK](https://www.nltk.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: In 1979, after more than a century of vaccination campaigns around the planet, the World Health Organization certified that smallpox had been eradicated\n",
      "(S\n",
      "  In/IN\n",
      "  1979/CD\n",
      "  ,/,\n",
      "  after/IN\n",
      "  more/JJR\n",
      "  than/IN\n",
      "  a/DT\n",
      "  century/NN\n",
      "  of/IN\n",
      "  vaccination/NN\n",
      "  campaigns/NNS\n",
      "  around/IN\n",
      "  the/DT\n",
      "  planet/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION World/NNP)\n",
      "  Health/NNP\n",
      "  Organization/NNP\n",
      "  certified/VBD\n",
      "  that/IN\n",
      "  smallpox/NN\n",
      "  had/VBD\n",
      "  been/VBN\n",
      "  eradicated/VBN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "sentences = sent_tokenize(phrase1)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence:\", sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags_sentence = pos_tag(words)\n",
    "    ne_chunks_sentence = ne_chunk(pos_tags_sentence)\n",
    "    print(ne_chunks_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam.\n",
      "(S\n",
      "  Beginning/VBG\n",
      "  in/IN\n",
      "  February/NNP\n",
      "  1965/CD\n",
      "  ,/,\n",
      "  there/EX\n",
      "  were/VBD\n",
      "  8/CD\n",
      "  weeks/NNS\n",
      "  of/IN\n",
      "  unbroken/JJ\n",
      "  bombing/NN\n",
      "  by/IN\n",
      "  (GPE U.S./NNP)\n",
      "  forces/NNS\n",
      "  of/IN\n",
      "  targets/NNS\n",
      "  in/IN\n",
      "  (GPE North/NNP Vietnam/NNP)\n",
      "  ./.)\n",
      "Sentence: Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.\n",
      "(S\n",
      "  Over/IN\n",
      "  the/DT\n",
      "  next/JJ\n",
      "  three/CD\n",
      "  years/NNS\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (GPE Unites/NNP States/NNPS)\n",
      "  dropped/VBD\n",
      "  more/JJR\n",
      "  bombs/NNS\n",
      "  than/IN\n",
      "  were/VBD\n",
      "  dropped/VBN\n",
      "  over/IN\n",
      "  (GPE Asia/NNP)\n",
      "  and/CC\n",
      "  (GPE Europe/NNP)\n",
      "  during/IN\n",
      "  World/NNP\n",
      "  War/NNP\n",
      "  II/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(phrase2)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence:\", sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags_sentence = pos_tag(words)\n",
    "    ne_chunks_sentence = ne_chunk(pos_tags_sentence)\n",
    "    print(ne_chunks_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. NER with [spaCy](https://spacy.io/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy currently provides support for the following [languages](https://spacy.io/usage/models). For applying NER for an English text, we need to download a spaCy NER model for [English](https://spacy.io/models/en). We choose to download `en_core_web_sm` because it is the smallest one in regards to download size (12Mb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: 1979 --- Entity Type (tag/label): DATE\n",
      "Entity: more than a century --- Entity Type (tag/label): DATE\n",
      "Entity: the World Health Organization --- Entity Type (tag/label): ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(phrase1)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print('Entity:', entity.text, '---', 'Entity Type (tag/label):', entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: February 1965 --- Entity Type: DATE\n",
      "Entity: 8 weeks --- Entity Type: DATE\n",
      "Entity: U.S. --- Entity Type: GPE\n",
      "Entity: North Vietnam --- Entity Type: LOC\n",
      "Entity: the next three years --- Entity Type: DATE\n",
      "Entity: the Unites States --- Entity Type: GPE\n",
      "Entity: Asia --- Entity Type: LOC\n",
      "Entity: Europe --- Entity Type: LOC\n",
      "Entity: World War II --- Entity Type: EVENT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(phrase2)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print('Entity:', entity.text, '---',  'Entity Type:', entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 : Beginning in February 1965, there were 8 weeks of unbroken bombing by U.S. forces of targets in North Vietnam.\n",
      "Sentence 1 : Over the next three years, the Unites States dropped more bombs than were dropped over Asia and Europe during World War II.\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print(\"Sentence\", i, ':', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IOB format (short for **i**nside, **o**utside, **b**eginning) is a common tagging format for tagging tokens in a chunking task in computational linguistics (ex. named entity recognition) to describe the entity boundaries. In IOB, the I- prefix before a tag indicates that the tag is inside a chunk. An O tag indicates that a token belongs to no chunk. The B- prefix before a tag indicates that the tag is the beginning of a chunk that immediately follows another chunk without O tags between them.\n",
    "\n",
    "|  | Named Entity  |  \n",
    "|-----|---|\n",
    "|  I   | An inner token of a multi-token entity |\n",
    "|  O   |  A non-entity token  |\n",
    "|  B   | The beginning token of a multi-token entity |\n",
    "\n",
    "| U.S.  | dropped | more | bombs | than | over | Asia  | and | Europe | during | World | War | II\n",
    "|---------|--------|---------|------|-------|------|---------|------|------|-------|-----|--------|--------|\n",
    "|  I-GPE       |    O    |    O     |  O    |    O   |   O   |   I-LOC      |   O   |  I-LOC    |    O   |   I-EVENT  |   I-EVENT     |    I-EVENT    |\n",
    "\n",
    "\n",
    "Another similar format which is widely used is IOB2 format, which is the same as the IOB format except that the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag). \n",
    "\n",
    "| U.S.  | dropped | more | bombs | than | over | Asia  | and | Europe | during | World | War | II\n",
    "|---------|--------|---------|------|-------|------|---------|------|------|-------|-----|--------|--------|\n",
    "|  B-GPE       |    O    |    O     |  O    |    O   |   O   |   B-LOC      |   O   |  B-LOC    |    O   |   B-EVENT  |   I-EVENT     |    I-EVENT    |\n",
    "\n",
    "\n",
    "Other tagging scheme is BIOES/BILOU, where 'E' and 'L' denotes Last or Ending token is such a sequence and 'S' denotes Single element or 'U' Unit element. \n",
    "\n",
    "|  | Named Entity  |  \n",
    "|-----|---|\n",
    "|  B   | The first token of a multi-token entity |\n",
    "|  I   | An inner token of a multi-token entity |\n",
    "|  L/E   |  The last/ending token of a multi-token entity  |\n",
    "|  O   |  A non-entity token  |\n",
    "|  U/S   | A single token entity |\n",
    "\n",
    "| U.S.  | dropped | more | bombs | than | over | Asia  | and | Europe | during | World | War | II\n",
    "|---------|--------|---------|------|-------|------|---------|------|------|-------|-----|--------|--------|\n",
    "|  S-GPE       |    O    |    O     |  O    |    O   |   O   |   S-LOC      |   O   |  S-LOC    |    O   |   B-EVENT  |   I-EVENT     |    E-EVENT    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [nltk.ne_chunk(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. How to build or train a NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General principles (ML, training data, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) State-of-the-art examples (list of SotA papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first end-to-end systems for sequence labeling tasks are based on pre-trained word and character embeddings encoded either by a bidirectional Long Short Term Memory (BiLSTM) network or a Convolutional Neural Network (CNN) ([Lample et al., 2016](https://arxiv.org/abs/1603.01360) and [Ma and Hovy 2016](https://arxiv.org/abs/1603.01354)), along with a Conditional Random Fields (CRF) decoder. One shortcoming of this type of model is that they were based on a single context-independent representation for each word. This problem has been further attenuated by methods based on language model pre-training that produced context-dependent word representations.\n",
    "These recent large-scale language models methods such as ELMo ([Peters et al. 2017](https://arxiv.org/abs/1705.00108)) and BERT ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)) further enhanced the performance of NER, yielding [state-of-the-art performances](http://nlpprogress.com/english/named_entity_recognition.html).\n",
    "\n",
    "- bidirectional Long Short Term Memory (BiLSTM) network [Lample et al., 2016](https://arxiv.org/abs/1603.01360)\n",
    "- Convolutional Neural Network (CNN) [Ma and Hovy 2016](https://arxiv.org/abs/1603.01354))\n",
    "- .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Use-case (mapping locations on a map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
